{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sent2vec\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cos\n",
    "from scipy.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load sentence embedding model\n",
    "# model_path = '../models/'\n",
    "path  = '/longterm/kaijil/data/10715/'\n",
    "model_path = path + 'models/'\n",
    "model_path = path + 'data/'\n",
    "\n",
    "model_wi_1 = sent2vec.Sent2vecModel()\n",
    "model_wi_1.load_model(model_path + 'wiki_unigrams.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_length_embedding(sents, labels, model):\n",
    "    \"\"\"\n",
    "    sents: array, n sentences\n",
    "    labels: array, label of every movie review\n",
    "    model: embedding model\n",
    "    return emb: embedding vectors of sorted sentences\n",
    "           sorted_labels\n",
    "           length\n",
    "    \"\"\"\n",
    "    dict_length = {}\n",
    "    tknzr = TweetTokenizer()\n",
    "    n = len(sents)\n",
    "    tokenized_sents = []\n",
    "    sentences = []\n",
    "    for i in range (n):\n",
    "        string = sents[i]\n",
    "        string = re.sub(r'[^\\w\\s]','',string)\n",
    "        sent_list = tknzr.tokenize(string)\n",
    "        dict_length[i] = len(sent_list)\n",
    "        tokenized_sents.append(' '.join(sent_list).lower())\n",
    "    \n",
    "    sorted_by_value = sorted(dict_length.items(), key=lambda kv: kv[1])\n",
    "    \n",
    "    sorted_sents = []\n",
    "    sorted_labels = []\n",
    "    length = []\n",
    "    for item in sorted_by_value:\n",
    "        sorted_sents.append(tokenized_sents[item[0]])\n",
    "        sorted_labels.append(labels[item[0]])\n",
    "        length.append(item[1])\n",
    "    emb = model.embed_sentences(sorted_sents)\n",
    "    return emb, sorted_labels, length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'rt-polaritydata/rt-polarity.neg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-71947cc3d9d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load movie reviews and preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmr_file_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rt-polaritydata/rt-polarity.neg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"latin-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmr_sent_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmr_file_neg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmr_file_neg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmr_file_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rt-polaritydata/rt-polarity.pos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"latin-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'rt-polaritydata/rt-polarity.neg'"
     ]
    }
   ],
   "source": [
    "# load movie reviews and preprocessing\n",
    "mr_file_neg = open(data_path + 'rt-polaritydata/rt-polarity.neg', encoding=\"latin-1\")\n",
    "mr_sent_neg = mr_file_neg.readlines()\n",
    "mr_file_neg.close()\n",
    "mr_file_pos = open('rt-polaritydata/rt-polarity.pos', encoding=\"latin-1\")\n",
    "mr_sent_pos = mr_file_pos.readlines()\n",
    "mr_file_pos.close()\n",
    "mr_sent_neg = np.array(mr_sent_neg)\n",
    "mr_sent_pos = np.array(mr_sent_pos)\n",
    "mr_y_neg = np.zeros_like(mr_sent_neg)\n",
    "for i in range(len(mr_y_neg)):\n",
    "    mr_y_neg[i] = 0\n",
    "mr_y_pos = np.ones_like(mr_sent_pos)\n",
    "mr_sent = np.concatenate((mr_sent_pos, mr_sent_neg))\n",
    "mr_y = np.concatenate((mr_y_pos, mr_y_neg))\n",
    "\n",
    "random.seed(2)\n",
    "random.shuffle(mr_sent)\n",
    "random.seed(2)\n",
    "random.shuffle(mr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_mr_x, sorted_mr_y, lengths = sort_length_embedding(mr_sent, mr_y, model_wi_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# logistic regression to classify the movie review\n",
    "def nestedCV(X, Y, Cs, innercv, outercv):\n",
    "    \"\"\"\n",
    "    Nested Cross Validation to select the best hyperparameters\n",
    "    and evaluate the logistic regression model.\n",
    "    :param X: n by d array, input features\n",
    "    :param Y: n by 1 array, labels\n",
    "    :param Cs: List or Array of candidates parameters for penalty in LR\n",
    "    :param innercv: int, fold of the inner cross validation\n",
    "    :param outercv: int, fold of the outer cross validation\n",
    "    :return: average score of cross validation\n",
    "    \"\"\"\n",
    "    clf_inner = GridSearchCV(estimator=LogisticRegression(), param_grid=Cs, cv=innercv)\n",
    "    clf_inner.fit(X, Y)\n",
    "    C_best = clf_inner.best_params_['C']\n",
    "    clf_outer = LogisticRegression(C=C_best)\n",
    "    scores = cross_val_score(clf_outer, X, Y, cv=outercv)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conduct_lr(x, y):\n",
    "    # classify the movie reviews and see the accuracy\n",
    "    sc = StandardScaler()\n",
    "    x_std = sc.fit_transform(x)\n",
    "\n",
    "    # create penalty coefficients candidates in logistic regression\n",
    "    C_candidates = dict(C=np.arange(5, 10, 1))\n",
    "\n",
    "    # nested CV for logistic regression\n",
    "    score = nestedCV(x_std, y, C_candidates, 3, 3)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n",
      "4413\n",
      "3228\n",
      "973\n",
      "[0.7467313340518599, 0.6955607161067222, 0.6422222222222222, 0.6388888888888888]\n"
     ]
    }
   ],
   "source": [
    "len_sep = [10, 20, 30, 55]\n",
    "indexes = []\n",
    "for i in range(len(len_sep)):\n",
    "    indexes.append([])\n",
    "\n",
    "for i in range(len(lengths)):\n",
    "    for pin in range(len(len_sep)):\n",
    "        if lengths[i] <= len_sep[pin]: \n",
    "            indexes[pin].append(i)\n",
    "            break\n",
    "\n",
    "mr_xs = []\n",
    "mr_ys = []\n",
    "\n",
    "for i in range(len(indexes)):\n",
    "    print(len(indexes[i]))\n",
    "\n",
    "for i in range(len(indexes)):\n",
    "    temp = np.random.choice(indexes[i], 900, replace=False)\n",
    "    mr_xs.append([sorted_mr_x[i] for i in temp])\n",
    "    mr_ys.append([sorted_mr_y[i] for i in temp])\n",
    "    \n",
    "accuracies = []\n",
    "for i in range(len(mr_xs)):\n",
    "    accuracies.append(conduct_lr(mr_xs[i], mr_ys[i]))\n",
    "\n",
    "\n",
    "print(accuracies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
