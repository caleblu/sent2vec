{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/longterm/kaijil/data/10715/models/\n"
     ]
    }
   ],
   "source": [
    "import sent2vec\n",
    "import glob\n",
    "from multiprocessing import Pool\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import codecs\n",
    "import nltk.data\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import numpy as np\n",
    "data_path  = '/longterm/kaijil/data/10715/'\n",
    "model_path = data_path + 'models/'\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model_wi_1 = sent2vec.Sent2vecModel()\n",
    "# model_wi_1.load_model(model_path + 'wiki_unigrams.bin')\n",
    "model_wi_2 = sent2vec.Sent2vecModel()\n",
    "model_wi_2.load_model(model_path + 'wiki_bigrams.bin')\n",
    "# model_tw_1 = sent2vec.Sent2vecModel()\n",
    "# model_tw_1.load_model(model_path + 'twitter_unigrams.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_tw_2 = sent2vec.Sent2vecModel()\n",
    "model_tw_2.load_model(model_path + 'twitter_bigrams.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model_tb_1 = sent2vec.Sent2vecModel()\n",
    "# model_tb_1.load_model(model_path + 'torontobooks_unigrams.bin')\n",
    "model_tb_2 = sent2vec.Sent2vecModel()\n",
    "model_tb_2.load_model(model_path + 'torontobooks_bigrams.bin')\n",
    "# model_gb_2 = sent2vec.Sent2vecModel()\n",
    "# model_gb_2.load_model(model_path + 'gutenbergbooks_bigrams.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# t1 = \"Ted Cruz should be disqualified from his fraudulent win in Iowa. Weak RNC and Republican leadership probably won't let this happen! Sad.\"\n",
    "# t2 = \"I wonder if President Obama would have attended the funeral of Justice Scalia if it were held in a Mosque? Very sad that he did not go!\"\n",
    "t1 = \"Kevin Stitt ran a great winning campaign against a very tough opponent in Oklahoma. Kevin is a very successful businessman who will be a fantastic Governor. He is strong on Crime & Borders, the 2nd Amendment, & loves our Military & Vets. He has my complete and total Endorsement!\"\n",
    "t2 = \"To the incredible people of the Great State of Wyoming: Go VOTE TODAY for Foster Friess - He will be a fantastic Governor! Strong on Crime, Borders & 2nd Amendment. Loves our Military & our Vets. He has my complete and total Endorsement!\"\n",
    "def get_similarity(t1,t2,model):\n",
    "    tknzr = TweetTokenizer()\n",
    "    t1 = ' '.join(tknzr.tokenize(t1)).lower()\n",
    "    t2 = ' '.join(tknzr.tokenize(t2)).lower()\n",
    "#     print(t1)\n",
    "#     print(t2)\n",
    "#     emb = model.embed_sentence(\"once upon a time .\") \n",
    "    emb = model.embed_sentences([t1,t2])\n",
    "#     print(emb.shape)\n",
    "    pearson = pearsonr(emb[0,:],emb[1,:])[0]\n",
    "    spearman = spearmanr(emb[0,:],emb[1,:])[0]\n",
    "    return np.round(pearson,3),np.round(spearman,3),np.round((pearson + spearman)/2.0,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.82, 0.8, 0.81)\n",
      "(0.84, 0.82, 0.83)\n",
      "(0.76, 0.73, 0.74)\n",
      "(0.83, 0.8, 0.81)\n",
      "(0.78, 0.76, 0.77)\n",
      "(0.77, 0.76, 0.76)\n",
      "(0.78, 0.77, 0.77)\n"
     ]
    }
   ],
   "source": [
    "print(get_similarity(t1,t2,model_wi_1))\n",
    "print(get_similarity(t1,t2,model_wi_2))\n",
    "print(get_similarity(t1,t2,model_tw_1))\n",
    "print(get_similarity(t1,t2,model_tw_2))\n",
    "print(get_similarity(t1,t2,model_tb_1))\n",
    "print(get_similarity(t1,t2,model_tb_2))\n",
    "print(get_similarity(t1,t2,model_gb_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bias in Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books Result for Driver bad driver| Race Asian people\n",
      "(0.158, 0.151, 0.154)\n",
      "Twitter Result for Driver bad driver| Race Asian people\n",
      "(0.112, 0.114, 0.113)\n",
      "Books Result for Driver bad driver| Race white people\n",
      "(0.078, 0.064, 0.071)\n",
      "Twitter Result for Driver bad driver| Race white people\n",
      "(0.093, 0.099, 0.096)\n",
      "Books Result for Driver bad driver| Race black people\n",
      "(0.082, 0.079, 0.08)\n",
      "Twitter Result for Driver bad driver| Race black people\n",
      "(0.055, 0.052, 0.054)\n",
      "Books Result for Driver driver| Race Asian people\n",
      "(0.124, 0.11, 0.117)\n",
      "Twitter Result for Driver driver| Race Asian people\n",
      "(0.083, 0.083, 0.083)\n",
      "Books Result for Driver driver| Race white people\n",
      "(0.069, 0.057, 0.063)\n",
      "Twitter Result for Driver driver| Race white people\n",
      "(0.073, 0.077, 0.075)\n",
      "Books Result for Driver driver| Race black people\n",
      "(0.065, 0.07, 0.067)\n",
      "Twitter Result for Driver driver| Race black people\n",
      "(0.034, 0.024, 0.029)\n",
      "Books Result for Driver good driver| Race Asian people\n",
      "(0.065, 0.06, 0.063)\n",
      "Twitter Result for Driver good driver| Race Asian people\n",
      "(0.026, 0.026, 0.026)\n",
      "Books Result for Driver good driver| Race white people\n",
      "(0.023, 0.004, 0.013)\n",
      "Twitter Result for Driver good driver| Race white people\n",
      "(0.062, 0.057, 0.059)\n",
      "Books Result for Driver good driver| Race black people\n",
      "(0.035, 0.04, 0.038)\n",
      "Twitter Result for Driver good driver| Race black people\n",
      "(0.016, 0.013, 0.015)\n"
     ]
    }
   ],
   "source": [
    "driver = ['bad driver','driver','good driver']\n",
    "races = ['Asian people','white people','black people']\n",
    "for d in driver:\n",
    "    for r in races:\n",
    "        print('Books Result for Driver ' + d + '| Race ' + r)\n",
    "        print(get_similarity(d,r,model_tb_2))\n",
    "        print('Twitter Result for Driver ' + d + '| Race ' + r)\n",
    "        print(get_similarity(d,r,model_tw_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = ['I am working hard','I am working hard','I am working h']\n",
    "races = ['']\n",
    "for d in driver:\n",
    "    for r in races:\n",
    "        print('Books Result for Driver ' + d + '| Race ' + r)\n",
    "        print(get_similarity(d,r,model_tb_2))\n",
    "        print('Twitter Result for Driver ' + d + '| Race ' + r)\n",
    "        print(get_similarity(d,r,model_tw_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
