{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sent2vec\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cos\n",
    "from scipy.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sentence embedding model\n",
    "model_path = '../models/'\n",
    "model_wi_1 = sent2vec.Sent2vecModel()\n",
    "model_wi_1.load_model(model_path + 'wiki_unigrams.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_length_embedding(sents1, sents2, labels, model):\n",
    "    \"\"\"\n",
    "    sents: array, n sentences\n",
    "    labels: array, label of every movie review\n",
    "    model: embedding model\n",
    "    return emb: embedding vectors of sorted sentences\n",
    "           sorted_labels\n",
    "           length\n",
    "    \"\"\"\n",
    "    dict_length = {}\n",
    "    tknzr = TweetTokenizer()\n",
    "    n = len(sents1)\n",
    "    tokenized_sents1 = []\n",
    "    tokenized_sents2 = []\n",
    "\n",
    "    for i in range (n):\n",
    "        string1 = sents1[i]\n",
    "        string2 = sents2[i]\n",
    "        string1 = re.sub(r'[^\\w\\s]','',string1)\n",
    "        string2 = re.sub(r'[^\\w\\s]','',string2)\n",
    "        sent_list1 = tknzr.tokenize(string1)\n",
    "        sent_list2 = tknzr.tokenize(string2)\n",
    "        dict_length[i] = (len(sent_list1)+len(sent_list2))/2.0\n",
    "        tokenized_sents1.append(' '.join(sent_list1).lower())\n",
    "        tokenized_sents2.append(' '.join(sent_list2).lower())\n",
    "    \n",
    "    sorted_by_value = sorted(dict_length.items(), key=lambda kv: kv[1])\n",
    "    \n",
    "    sorted_sents1 = []\n",
    "    sorted_sents2 = []\n",
    "    sorted_labels = []\n",
    "    length = []\n",
    "    \n",
    "    for item in sorted_by_value:\n",
    "        sorted_sents1.append(tokenized_sents1[item[0]])\n",
    "        sorted_sents2.append(tokenized_sents2[item[0]])\n",
    "        sorted_labels.append(labels[item[0]])\n",
    "        length.append(item[1])\n",
    "    emb1 = model.embed_sentences(sorted_sents1)\n",
    "    emb2 = model.embed_sentences(sorted_sents2)\n",
    "    \n",
    "    return emb1, emb2, sorted_labels, length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate STS using cosine similarity\n",
    "# and compare the results with the gold standard.\n",
    "# sentsets: sentence datasets:\n",
    "#           deft-forum, deft-news, headlines, images, OnWM, tweet-news\n",
    "def STS_eval(sentset, model):\n",
    "    \"\"\"\n",
    "    Evaluate the similarities of \n",
    "    :param sentset: string, sentence dataset\n",
    "    :param model: sentence embedding model\n",
    "    :return: cosine similarity, of all pairs of sentences\n",
    "             pearson & spearman coefficients compared to gold standard\n",
    "    \"\"\"\n",
    "    sent_file = open('sts-en-test-gs-2014/STS.input.'+sentset+'.txt')\n",
    "    sent_data = sent_file.readlines()\n",
    "    sent_file.close()\n",
    "    gs_file = open('sts-en-test-gs-2014/STS.gs.'+sentset+'.txt')\n",
    "    gs_data = np.array(gs_file.readlines(), dtype=float)\n",
    "    gs_file.close()\n",
    "    splited_sent = []\n",
    "    n = len(sent_data)\n",
    "    for i in range(n):\n",
    "        splited_sent.append(re.split(r'\\t+', sent_data[i]))\n",
    "    splited_sent = np.array(splited_sent)\n",
    "    sent_1 = splited_sent[:,0]\n",
    "    sent_2 = splited_sent[:,1]\n",
    "    x_1, x_2, y, ls = sort_length_embedding(sent_1, sent_2, gs_data, model)\n",
    "    \n",
    "    s1 = x_1[:81]\n",
    "    s2 = x_2[:81]\n",
    "    y1 = y[:81]\n",
    "    c1 = []\n",
    "    \n",
    "    s1_2 = x_1[81:162]\n",
    "    s2_2 = x_2[81:162]\n",
    "    y2 = y[81:162]\n",
    "    c2 = []\n",
    "    \n",
    "    s1_3 = x_1[162:227]\n",
    "    s2_3 = x_2[162:227]\n",
    "    y3 = y[162:227]\n",
    "    c3 = []\n",
    "    \n",
    "    s1_4 = x_1[227:]\n",
    "    s2_4 = x_2[227:]\n",
    "    y4 = y[227:]\n",
    "    c4 = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    pearsons = []\n",
    "    spearmanrs = []\n",
    "    \n",
    "\n",
    "    for i in range(len(s1)):\n",
    "        v1 = s1[i]\n",
    "        v2 = s2[i]\n",
    "        cos_i = cos([v1], [v2])\n",
    "        c1.append(cos_i[0][0])\n",
    "    pearsons.append(pearsonr(c1, y1)[0])\n",
    "    spearmanrs.append(spearmanr(c1, y1)[0])\n",
    "    \n",
    "    for i in range(len(y2)):\n",
    "        v1 = s1_2[i]\n",
    "        v2 = s2_2[i]\n",
    "        cos_i = cos([v1], [v2])\n",
    "        c2.append(cos_i[0][0])\n",
    "    pearsons.append(pearsonr(c2, y2)[0])\n",
    "    spearmanrs.append(spearmanr(c2, y2)[0])\n",
    "    \n",
    "    for i in range(len(y3)):\n",
    "        v1 = s1_3[i]\n",
    "        v2 = s2_3[i]\n",
    "        cos_i = cos([v1], [v2])\n",
    "        c3.append(cos_i[0][0])\n",
    "    pearsons.append(pearsonr(c3, y3)[0])\n",
    "    spearmanrs.append(spearmanr(c3, y3)[0])\n",
    "    \n",
    "    for i in range(len(y4)):\n",
    "        v1 = s1_4[i]\n",
    "        v2 = s2_4[i]\n",
    "        cos_i = cos([v1], [v2])\n",
    "        c4.append(cos_i[0][0])\n",
    "    pearsons.append(pearsonr(c4, y4)[0])\n",
    "    spearmanrs.append(spearmanr(c4, y4)[0])\n",
    "        \n",
    "    \n",
    "    return pearsons, spearmanrs\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spearman correlation with gs: [0.7077972836577799, 0.721828545619389, 0.63255127702153, 0.29036327210484564]\n",
      "pearson correlation with gs: [0.6940241048974188, 0.7504630506091284, 0.7369924567923407, 0.37552997030724206]\n"
     ]
    }
   ],
   "source": [
    "pearson_news, spearman_news = STS_eval('deft-news', model_wi_1)\n",
    "\n",
    "print('spearman correlation with gs:',  spearman_news)\n",
    "print('pearson correlation with gs:', pearson_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
